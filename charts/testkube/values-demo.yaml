# Demo values for testkube.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  nodeSelector:
    cloud.google.com/gke-provisioning: standard
  tolerations:
    - key: kubernetes.io/arch
      operator: Equal
      value: arm64
      effect: NoSchedule
  features:
    logsV2: false

replicaCount: 1

image:
  repository: ""
  pullPolicy: Never
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: "testkube"
fullnameOverride: "testkube"

## Custom job-template.yml that will passed to Testkube API
configValues: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: "false"
  className: ""
  annotations:
    {}
    # kubernetes.io/ingress.class: nginx
  # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources:
  {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
#   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# For more configuration parameters of MongoDB chart please look here:
# https://github.com/bitnami/charts/tree/master/bitnami/mongodb#parameters
mongodb:
  enabled: true
  nameOverride: "mongodb"
  fullnameOverride: "testkube-mongodb"
  architecture: "standalone"
  auth:
    enabled: false
    # rootPassword: "123DefaultOne321"
  service:
    port: "27017"
    portName: "mongodb"
    nodePort: true
    clusterIP: ""
  priorityClassName: "highest-priority-mongodb"
  resources:
    requests:
      memory: "1500Mi"
      cpu: "200m"
  nodeSelector:
    cloud.google.com/gke-provisioning: standard
  image:
    registry: docker.io
    repository: zcube/bitnami-compat-mongodb
    tag: 6.0.5-debian-11-r64
  updateStrategy:
    type: Recreate

# -- NATS pre-upgrade parameters
preUpgradeHookNATS:
  # -- Upgrade hook is enabled
  enabled: true
  # -- Upgrade hook name
  name: nats-upgrade
  ## --  TTL (time to live) mechanism to limit the lifetime of Job objects that have finished execution, specified in seconds
  ttlSecondsAfterFinished: 100
  ## -- Specific labels
  labels: {}
  ## -- Annotations to add to the upgrade Job
  annotations: {}
  ## -- Annotations to add to the upgrade Job's pod
  podAnnotations: {}
  # -- Specify image
  image:
    registry: docker.io
    repository: bitnami/kubectl
    tag: 1.28.2
    pullPolicy: IfNotPresent
    pullSecrets: []
  # -- Specify resource limits and requests
  resources: {}
  # -- Create SA for upgrade hook
  serviceAccount:
    create: true
  # -- Node labels for pod assignment.
  nodeSelector: {}
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  # -- MongoDB Upgrade Pod Security Context
  podSecurityContext: {}
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  # -- Security Context for MongoDB Upgrade kubectl container
  securityContext: {}
  # ref: https://cloud.google.com/kubernetes-engine/docs/how-to/prepare-arm-workloads-for-deployment#node-affinity-multi-arch-arm
  # -- Tolerations to schedule a workload to nodes with any architecture type. Required for deployment to GKE cluster.
  tolerations: []

# -- NATS chart parameters
nats:
  # NATS container settings
  podTemplate:
    merge:
      spec:
        nodeSelector:
          cloud.google.com/gke-provisioning: standard
  config:
    merge:
      max_payload: << 8MB >>
    jetstream:
      enabled: true

  # NATS Box container settings
  # TODO remove this container after tests on dev and stage
  # nats-box is A lightweight container with NATS utilities. It's not needed for nats server
  natsBox:
    enabled: false
    podTemplate:
      merge:
        spec:
          nodeSelector:
            cloud.google.com/gke-provisioning: standard

testkube-api:
  rbac:
    createRoles: true
    createRoleBindings: true
  prometheus:
    enabled: false
  nameOverride: "api-server"
  fullnameOverride: "testkube-api-server"
  image:
    repository: kubeshop/testkube-api-server
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    # TODO we should stick to static version
    # tag: "latest"
  service:
    type: ClusterIP
    port: 8088
  minio:
    enabled: true
    tolerations: []
    affinity: {}
    priorityClassName: "highest-priority"
    storage: 80Gi
    image:
      registry: docker.io
      repository: minio/minio
      tag: RELEASE.2023-09-16T01-01-47Z
    resources:
      requests:
        memory: "1000Mi"
        cpu: "200m"

  uiIngress:
    enabled: false
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/rewrite-target: /$1
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/enable-cors: "true"
      nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST"
      nginx.ingress.kubernetes.io/cors-allow-credentials: "false"
      # specify the name of the global IP address resource to be associated with the HTTP(S) Load Balancer.
      kubernetes.io/ingress.global-static-ip-name: testkube-demo
      # add an annotation indicating the issuer to use.
      # cert-manager.io/cluster-issuer: letsencrypt-prod
      # controls whether the ingress is modified ‘in-place’,
      # or a new one is created specifically for the HTTP01 challenge.
      # acme.cert-manager.io/http01-edit-in-place: "true"

      # for websockets
      nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"

      nginx.ingress.kubernetes.io/server-snippet: |
        set $methodallowed "";
        set $pathallowed "";

        if ( $request_method = GET ){
          set $methodallowed "true";
          set $pathallowed "true";
        }

        if ( $request_method = POST ){
          set $methodallowed "true";
        }

        if ( $request_method = OPTIONS ){
          set $methodallowed "true";
        }

        if ( $request_method = PATCH ){
          set $methodallowed "true";
        }

        if ( $uri ~ "^(.*)/tests/(.*)/executions$" ){
          set $pathallowed "true";
        }

        if ( $uri ~ "^(.*)/tests/(.*)/executions/(.*)$" ){
          set $pathallowed "true";
        }

        if ( $uri ~ "^(.*)/test-suites/(.*)/executions$" ){
          set $pathallowed "true";
        }

        if ( $uri ~ "^(.*)/test-suites/(.*)/executions/(.*)$" ){
          set $pathallowed "true";
        }

        if ( $uri ~ "^(.*)/tests$" ){
          set $pathallowed "true";
        }

        if ( $uri ~ "^(.*)/tests/(.*)$" ){
          set $pathallowed "true";
        }

        if ( $uri ~ "^(.*)/test-suite-executions/(.*)$" ){
          set $pathallowed "true";
        }

        if ( $uri ~ "^(.*)/repositories$" ){
          set $pathallowed "true";
        }

        set $condition "$methodallowed+$pathallowed";
        if ( $condition != "true+true" ) {
          return 401;
        }

    path: /results/(v\d/.*)
    hosts:
      - demo.testkube.io
    tlsenabled: "true"
    tls: # < placing a host in the TLS config will indicate a certificate should be created
      - hosts:
          - demo.testkube.io
        secretName: testkube-demo-cert-secret
  cliIngress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/rewrite-target: /$1
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_input_headers "X-CLI-Ingress: true";
    # parameters to check oauth token (by default github one)
    oauth:
      clientID: ""
      clientSecret: ""
      provider: "github"
      scopes: ""
    path: /api/(v\d/.*)
    hosts:
      - demo.testkube.io
    tlsenabled: "false"
    tls: # < placing a host in the TLS config will indicate a certificate should be created
      - hosts:
          - demo.testkube.io
        secretName: testkube-demo-cert-secret
  storage:
    endpoint: ""
    endpoint_port: "9000"
    accessKeyId: "minio"
    accessKey: "minio123"
    region: ""
    token: ""
    bucket: "testkube-artifacts"
    expiration: 90
    SSL: false
    scrapperEnabled: true
    compressArtifacts: true

  ## Logs storage for Testkube API.
  logs:
    ## where the logs should be stored there are 2 possible valuse : minio|mongo
    storage: "minio"
    ## if storage is set to minio then the bucket must be specified, if minio with s3 is used make sure to use a unique name
    bucket: "testkube-logs"

  mongodb:
    dsn: "mongodb://testkube-mongodb:27017"
    # or you can pass mongo dsn from secret
    # secretName: testkube-secrets
    # secretKey: mongo-dsn
    allowDiskUse: true

  ## Set custom livenessProbe
  livenessProbe:
    ## Probe type
    httpGet:
      path: /health
      port: http
      scheme: HTTP
    ## Amount of request failures before the container receives a terminate signal
    failureThreshold: 3
    ## Time to wait after the initial deployment before performing first probe
    initialDelaySeconds: 180
    ## How often (in seconds) to perform the probes. This value sends one probe every 30 seconds
    periodSeconds: 30
    ## Minimum consecutive successes for the probe to be considered successful after having failed
    successThreshold: 1
    ## Number of seconds after which the probe times out
    timeoutSeconds: 10

  ## Set custom readinessProbe
  readinessProbe:
    ## Probe type
    httpGet:
      path: /health
      port: http
      scheme: HTTP
    ## Amount of request failures before the container receives a terminate signal
    failureThreshold: 3
    ## Time to wait after the initial deployment before performing first probe
    initialDelaySeconds: 60
    ## How often (in seconds) to perform the probes. This value sends one probe every 30 seconds
    periodSeconds: 30
    ## Minimum consecutive successes for the probe to be considered successful after having failed
    successThreshold: 1
    ## Number of seconds after which the probe times out
    timeoutSeconds: 10

  resources:
    requests:
      memory: "200Mi"
      cpu: "200m"

  analyticsEnabled: true
  podStartTimeout: "30m"
  slackToken: ""
  slackSecret: ""
  slackTemplate: ""
  slackConfig: ""
  executors: ""
  cdeventsTarget: ""
  dashboardUri: ""
  clusterName: ""
  enableSecretsEndpoint: false
  disableSecretCreation: false
  defaultStorageClassName: ""
  enableK8sEvents: true

  testkubeLogs:
    grpcAddress: "testkube-logs:9090"

  priorityClassName: "highest-priority"

  ##Test Connection pod
  testConnection:
    enabled: true

  # -- Executors' image tag
  executor:
    image:
      tag: ""

testkube-operator:
  # should roles and roles bindings be created
  rbac:
    createRoles: true
    createRoleBindings: true

  # should the CRDs be installed
  installCRD: true
  priorityClassName: "highest-priority"

  ##Proxy Image parameters
  ## image.registry Proxy image registry
  ## image.repository Proxy image name
  ## image.tag Proxy image tag
  ## image.pullPolicy Proxy Image pull policy
  proxy:
    image:
      registry: gcr.io
      repository: kubebuilder/kube-rbac-proxy
      tag: "v0.8.0"
    ## Proxy Container Port
    containerPort: 8443

  resources:
    requests:
      memory: "200Mi"
      cpu: "100m"

  ##Test Connection pod
  testConnection:
    enabled: true
